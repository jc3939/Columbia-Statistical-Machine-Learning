library(MASS)
#This part generates the sample data set.
fakedata=function(w, n_sample){
  d=2
  sigma=matrix(c(0.2,0.1,0.2,0.1),2,2)
  data_set_one=mvrnorm(n_sample,c(1,0),sigma)
  data_set_two=mvrnorm(n_sample,c(-1,0),sigma)
  data_set=rbind(data_set_one,data_set_two)
  data_set=data_set[sample.int(dim(data_set)[1]),]
  ones=matrix(rep(1,2*n_sample),2*n_sample,1)
  S=cbind(data_set,ones)
  y=S%*%w
  y[y>0]=1
  y[y<0]=-1
  output=list(a=S,b=y)
  return(output)
}

#This part is to evaluate a Perceptron solution
classify=function(S,z){
  y=S%*%z
  y[y>0]=1
  y[y<0]=-1
  return(y)
}

# Count the number of different results generated by training algorithm f and the actual value y.
count=function(f,y){
  result=f-y
  result[result==2]=1
  result[result==-2]=1
  return(result)
}

#this is the batch version of Perceptron training algorithm.
perceptrain=function(S, y){
  z=matrix(c(1,-1,0),3,1)
  z_history=z
  print(z)
  cost_function=t(abs(classify(S,z)-y))%*%(S%*%z)
  k=1
  while (abs(cost_function)!=0){
    f=classify(S,z)
    z=z-(1/k)*t(t(count(f,y)*(-y))%*%S)
    class=classify(S,z)
    cost_function=t(0.5*abs(class-y))%*%(S%*%z)
    print(cost_function)
    k=k+1
    z_history=cbind(z_history,z)
  }
  output=list(a=z,b=z_history)
  return(output)
}

#This is the fixed increment single sample version of Perceptron training
simple_perceptrain=function(S, y){
  z=matrix(c(1,-1,0),3,1)
  z_history=z
  #print(z)
  cost_function=t(abs(classify(S,z)-y))%*%(S%*%z)
  i=1
  while (abs(cost_function)!=0){
    if (i==200){
      i=1
    }
    class=classify(S,z)
    if (class[i]!=y[i]){
      z=z+y[i]*S[i,]
      #i=i+1
    }

      i=i+1
      cost_function=t(0.5*abs(class-y))%*%(S%*%z)
      #print(cost_function)
      z_history=cbind(z_history,z)
    
    print(cost_function)
  }
  output=list(a=z,b=z_history)
  return(output)
}
###############################################################################################
#This part is to generate sample training data.
w=c(sample(1:0.1:2,2),0)
out=fakedata(w,100)
S=out$a
y=out$b

#This part is to implement the batch training algorithm and plot the scatter graph and the normal vector Z.

out=perceptrain(S,y)
z=out$a
z_history=out$b
plot(S)
lines(c(1,-1),c((1*z[1]-z[3])/z[2],(-1*z[1]-z[3])/z[2]),col="red",lwd=10)


#Generate test sample data to check if this algorithm is working correctly.
test=fakedata(w,100)
test_data=test$a
test_label=test$b

#I calculate the cost function based on test data and hyperplane normal vector(z), and the cost function is 0,
# which means my algorithm is working.
cost_function=t(abs(classify(test_data,z)-test_label))%*%(test_data%*%z)
plot(test_data)

#In this part, I plot the trajectory of algorithm by visualizing the Z_history.
for (col in 1:dim(z_history)[2]){
  curr_z=z_history[,col]
  lines(c(1,-1),c((1*curr_z[1]-curr_z[3])/curr_z[2],(-1*curr_z[1]-curr_z[3])/curr_z[2]))
}
lines(c(1,-1),c((1*z[1]-z[3])/z[2],(-1*z[1]-z[3])/z[2]),col="red",lwd=10)

###########################################################################################

#run the fixed increment algorithm and test its performance, which is also correct.
simple_out=simple_perceptrain(S,y)
z2=simple_out$a
z_history2=simple_out$b
cost_function2=t(abs(classify(test_data,z2)-test_label))%*%(test_data%*%z2)
plot(S)
lines(c(1,-1),c((1*z2[1]-z2[3])/z2[2],(-1*z2[1]-z2[3])/z2[2]),col="red",lwd=10)
for (col in 1:dim(z_history2)[2]){
  curr_z=z_history2[,col]
  lines(c(1,-1),c((1*curr_z[1]-curr_z[3])/curr_z[2],(-1*curr_z[1]-curr_z[3])/curr_z[2]))
}





